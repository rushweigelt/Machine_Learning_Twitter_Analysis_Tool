{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "antiracist_try2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVOHRGqGegVHp7Vy9ejexf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushweigelt/Machine_Learning_Twitter_Analysis_Tool/blob/master/openai_antiracistbot/antiracist_try2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCsMvItW8yvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "06bf60b6-54ef-4841-9724-933539680853"
      },
      "source": [
        "#mount personal google drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ufTvhS-8-0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make a few directories for later, then most importantly ocme back to content for permission reasons of all thoings\n",
        "#%cd drive\n",
        "#%cd My\\ Drive\n",
        "#%mkdir bot\n",
        "%cd /content/\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCypZ0bSk8aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1AOzF-59Q83",
        "colab_type": "code",
        "outputId": "2ec9f1f7-f339-4530-dc09-c5d17ab129b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#Clone into /content\n",
        "!git clone https://github.com/mohamad-ali-nasser/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Total 395 (delta 0), reused 0 (delta 0), pack-reused 395\u001b[K\n",
            "Receiving objects: 100% (395/395), 4.43 MiB | 6.76 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5FYg2t-9ttj",
        "colab_type": "code",
        "outputId": "9717e258-60f8-45a7-8813-cb3e6b7369c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#grab medium\n",
        "%cd gpt-2\n",
        "!python3 download_model.py 345M"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n",
            "Fetching checkpoint: 1.00kit [00:00, 743kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 57.1Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 780kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:20, 69.6Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 5.18Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 47.2Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 38.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCN_o9ylgY7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#too large for free collab notbooks :(\n",
        "#!python3 download_model.py 774M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzZdx7yW9y4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load old checkpoints\n",
        "!cp -r /content/drive/My\\ Drive/bot/checkpoint/run1/* /content/gpt-2/models/345M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7qfB3SD5XNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load previous checkpoint\n",
        "#!cp -r /content/drive/My\\ Drive/bot/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o20pRxM-906F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make folder to drop data into\n",
        "%cd src\n",
        "%mkdir sample_data\n",
        "%cd sample_data\n",
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzFFLKK192vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#go back to outer gtp-2 and then install reqs\n",
        "%cd ..\n",
        "%cd ..\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0s6KTLg-Pls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get right version of tf\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install 'tensorflow-estimator<1.15.0rc0,>=1.14.0rc0' --force-reinstall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN8_bWgF-XN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Insane ass cuda stuff, bleh, cuda needs to be such an exact version\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-*/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9-0\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_OEzPWb-Zhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%cd gpt-2\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNu-wbfn-bhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#where we train, run from gpt-2\n",
        "!PYTHONPATH=src ./train.py --dataset src/sample_data/s.txt --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EVGVCkq-2ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy checkpoint to personal drive and the model\n",
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/bot/checkpoint\n",
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTTGlqiXMkoe",
        "colab_type": "text"
      },
      "source": [
        "Now that model has been updated, lets interact\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmE2eHDaLWe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#skip cell generally\n",
        "import os\n",
        "%cd src\n",
        "from conditional_model import conditional_model\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkMyl5qzN69r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#skip cell\n",
        "conditional_model(seed=1,sentences=['How are you today?', 'Hi i am here'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bqYcZC8MozF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#start back here\n",
        "%cd src\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XU9GW64Mq_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqCvjf7Y_MOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k6vrI4_MwKI",
        "colab_type": "code",
        "outputId": "cd203e3d-9149-4866-eae9-78a3b80bbf5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        }
      },
      "source": [
        "#needs to be run from gpt-2 directory\n",
        "interact_model(\n",
        "    '345M',\n",
        "    None,\n",
        "    1,\n",
        "    1,\n",
        "    100,\n",
        "    1,\n",
        "    0,\n",
        "    '/content/gpt-2/models'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "INFO:tensorflow:Restoring parameters from /content/gpt-2/models/345M/model.ckpt\n",
            "Model prompt >>> What is white nationalism?\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "White nationalism says that people should be united and hate getting disapproved too much by others: they should feel they are equal because they don't want to get punished or annoyed. When people influence your life, everyone will be affected; if there is someone who annoys you, you can't just stay silent about it etc etc. Therefore, in the white politician presence it is dangerous because it requires a protest and takes many people out of everyday life.\n",
            "\n",
            "What is the state of the\n",
            "================================================================================\n",
            "Model prompt >>> Why is white nationalism so common in the United States?\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "It's morally wrong.\n",
            "\n",
            "Because blacks in America are almost universally less educated and disproportionately poor and exploited. We cannot go into front lines as warriors without being watched, stopped and followed. ObamaCare became impossible because the insurance companies wish it were racist or fixable. Many of the hundreds of thousands of snakes and beetles (adultery?) victims of Black Mafia Politicians for our benefit are dumb enough to simply dismiss them–and should we tolerate allies of sorts in the appointments office who promote\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}